# -*- coding: utf-8 -*-
"""resig_bal+reload

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j8_l0HDTmZQVjfGd7e-pQSRMhNkYdhmt
"""

!pip install pandas_market_calendars

import pandas_market_calendars as mcal
from google.colab import files
#uploaded = files.upload()
import io
import requests
import numpy as np  
import pandas as pd 
#from pandas.io.json import json_normalize
from datetime import datetime, timedelta

import sklearn
#print(sklearn.__version__)
import matplotlib.pyplot as plt
#import scipy as sp 
from sklearn.model_selection import train_test_split
#from sklearn.preprocessing import Imputer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
#from sklearn.ensemble import RandomForestRegressor
from sklearn import metrics
#from sklearn.preprocessing import PolynomialFeatures
#from sklearn.feature_selection import SelectKBest
#from sklearn.feature_selection import f_regression
from sklearn.model_selection import GridSearchCV
#from sklearn.naive_bayes import BernoulliNB
import sklearn.pipeline as skpipeline
#from sklearn.feature_selection import RFECV
from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor
#from sklearn.linear_model import LogisticRegression
#from sklearn.svm import LinearSVC
from sklearn.feature_selection import SelectFromModel
#from sklearn.svm import SVC
from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA
#from sklearn.model_selection import StratifiedKFold
#from sklearn.pipeline import FeatureUnion
from sklearn.neural_network import MLPClassifier
#from sklearn.naive_bayes import GaussianNB
#from sklearn.base import BaseEstimator, TransformerMixin
#from sklearn.preprocessing import MultiLabelBinarizer
#from skmultilearn.problem_transform import BinaryRelevance
#from sklearn.preprocessing import LabelBinarizer
#from sklearn.multiclass import OneVsRestClassifier
from sklearn.model_selection import cross_val_score
import pickle
import joblib
#from joblib import dump, load
#from sklearn.model_selection import ShuffleSplit
#import dask.dataframe as dd
#from dask_ml import model_selection
import xgboost as xgb
import math

import random
from tempfile import mkdtemp
from shutil import rmtree
import gc

#from sklearn.datasets import make_classification

"""##Setup
- date range & tickers
- function to pull data from Alpaca & create Dataframe
"""

# Ending_date set to today (-24 hours because colab on UTC)
ending_date = datetime.today() - timedelta(hours=24) 
end_date = ending_date.strftime('%Y-%m-%d')
end_date = end_date + "T09:30:00-04:00" #formatting needed for api call

nyse = mcal.get_calendar('NYSE')

# Trading days since 2014
date_range = nyse.schedule(start_date = '2014-10-01', end_date = ending_date.strftime('%Y-%m-%d'))
                          #end_date='2019-10-06')

data_limit = 400 #600 days trading data days limit, data pull x2 to get 1200 total trading data, can be changed 
date_range=date_range.loc[::-1].reset_index()
ending=date_range.iloc[data_limit][0].strftime('%Y-%m-%d')
ending = ending + "T09:30:00-04:00" #formatting needed for api call
print(ending)

# Q1500 tickers from Quantopian; limit requires multiple requests
Q1500_1="A,AA,AAL,AAN,AAON,AAP,AAPL,AAT,AAXN,ABBV,ABC,ABCB,ABG,ABM,ABMD,ABR,ABT,ACA,ACAD,ACB,ACC,ACCO,ACGL,ACHC,ACIA,ACIW,ACM,ACN,ADBE,ADC,ADI,ADM,ADNT,ADP,ADPT,ADS,ADSK,ADSW,ADT,ADUS,AEE,AEIS,AEL,AEM,AEO,AEP,AER,AERI,AES,AFG,AFIN,AFL,AG,AGCO,AGI,AGIO,AGN,AGNC,AGO,AGR,AIG,AIMC,AIMT,AIN,AINV,AIR,AIT,AIV,AIZ,AJG,AJRD,AKAM,AKBA,AKCA,AKR,AKS,AL,ALB,ALE,ALEC,ALEX,ALG,ALGN,ALGT,ALK,ALKS,ALL,ALLE,ALLK,ALLO,ALLY,ALNY,ALRM,ALSN,ALTR,ALV,ALXN,AM,AMAT,AMBA,AMBC,AMCR,AMCX,AMD,AME,AMED,AMG,AMGN,AMH,AMKR,AMN,AMP,AMRX,AMT,AMTD,AMWD,AMZN,AN,ANET,ANF,ANGI,ANSS,ANTM,AON,AOS,APA,APAM,APD,APH,APHA,APLE,APLS,APOG,APPF,APPN,APTV,APY,AQN,AQUA,AR,ARCC,ARCH,ARE,ARES,ARGO,ARI,ARMK,ARNA,ARNC,AROC,ARQL,ARR,ARVN,ARW,ARWR,ASB,ASGN,ASH,ASTE,ATGE,ATH,ATI,ATKR,ATNX,ATO,ATR,ATRA,ATRC,ATRI,ATRO,ATSG,ATUS,ATVI,AUB,AUPH,AUY,AVA,AVAV,AVB,AVGO,AVLR,AVNS,AVT,AVTR,AVX,AVY,AVYA,AWI,AWK,AWR,AX,AXE,AXL,AXNX,AXP,AXS,AXSM,AXTA,AY,AYI"
Q1500_2="AYX,AZO,AZPN,AZZ,B,BA,BAC,BAH,BAM,BANC,BAND,BANR,BAP,BAX,BB,BBBY,BBY,BC,BCC,BCE,BCO,BCOR,BCPC,BDC,BDN,BDX,BE,BEAT,BECN,BEN,BERY,BF.B,BFAM,BG,BGCP,BGS,BHC,BHE,BHF,BHLB,BHVN,BIG,BIIB,BIO,BJ,BJRI,BK,BKD,BKE,BKH,BKI,BKNG,BKR,BKU,BL,BLD,BLDP,BLDR,BLK,BLKB,BLL,BLMN,BLUE,BMCH,BMI,BMO,BMRN,BMY,BNS,BOH,BOKF,BOLD,BOOT,BOX,BPFH,BPMC,BPOP,BPR,BR,BRC,BRK.B,BRKR,BRKS,BRO,BRX,BSIG,BSX,BTG,BTU,BURL,BWA,BWXT,BX,BXMT,BXP,BXS,BYD,BYND,C,CABO,CACC,CACI,CADE,CAE,CAG,CAH,CAKE,CAL,CALM,CAR,CARG,CARS,CASH,CASY,CAT,CATM,CATY,CB,CBOE,CBPO,CBRE,CBRL,CBSH,CBT,CBU,CBZ,CC,CCC,CCEP,CCI,CCJ,CCK,CCL,CCMP,CCO,CCOI,CCS,CCXI,CDAY,CDE,CDEV,CDK,CDLX,CDNA,CDNS,CDW,CDXS,CE,CEF,CENTA,CERN,CF,CFG,CFR,CFX,CG,CGBD,CGC,CGNX,CHCT,CHD,CHDN,CHE,CHEF,CHGG,CHH,CHK,CHKP,CHRS,CHRW,CHTR,CHWY,CI,CIEN,CIM,CINF,CIR,CISN,CIT,CL,CLB,CLDR,CLF,CLGX,CLH,CLI,CLNC,CLNY,CLR,CLX,CM,CMA,CMC,CMCO,CMCSA,CMD,CME,CMG,CMI,CMO"
Q1500_3="CMP,CMPR,CMS,CNA,CNC,CNDT,CNHI,CNI,CNK,CNMD,CNNE,CNO,CNP,CNQ,CNR,CNS,CNX,CODI,COF,COG,COHR,COHU,COKE,COLB,COLD,COLM,COMM,CONE,COO,COOP,COP,COR,CORE,CORT,COST,COT,COTY,COUP,CP,CPA,CPB,CPE,CPG,CPK,CPRI,CPRT,CPT,CR,CREE,CRI,CRL,CRM,CRON,CROX,CRS,CRSP,CRUS,CRVL,CRWD,CRY,CSCO,CSFL,CSGP,CSGS,CSII,CSIQ,CSL,CSOD,CSTM,CSX,CTAS,CTB,CTL,CTLT,CTRE,CTSH,CTVA,CTXS,CUB,CUBE,CUZ,CVA,CVBF,CVCO,CVE,CVET,CVGW,CVI,CVLT,CVNA,CVS,CVX,CW,CWEN,CWH,CWK,CWST,CWT,CXO,CXP,CXW,CY,CYBR,CZR,CZZ,D,DAL,DAN,DAR,DB,DBD,DBI,DBX,DCI,DCPH,DD,DDD,DDOG,DDS,DE,DEA,DECK,DEI,DELL,DENN,DERM,DESP,DFS,DG,DGX,DHC,DHI,DHR,DHT,DIN,DIOD,DIS,DISCA,DISH,DK,DKS,DLB,DLPH,DLR,DLTR,DLX,DNKN,DNLI,DNOW,DO,DOC,DOCU,DOOR,DORM,DOV,DOW,DOX,DPZ,DRE,DRH,DRI,DRNA,DRQ,DSL,DT,DTE,DUK,DVA,DVN,DXC,DXCM,DY,EA,EAF,EAT,EB,EBAY,EBIX,EBS,ECA,ECL,ECOL,ECPG,ED,EDIT,EE,EEFT,EFX,EGBN,EGHT,EGO,EGOV,EGP,EGRX,EHC,EHTH,EIDX,EIG,EIX,EL"
Q1500_4="ELAN,ELF,ELS,ELY,EME,EMN,EMR,ENB,ENDP,ENLC,ENPH,ENR,ENS,ENSG,ENTA,ENTG,ENV,ENVA,EOG,EPAC,EPAM,EPAY,EPC,EPR,EPRT,EPZM,EQC,EQH,EQIX,EQR,EQT,ERF,ERI,ERIE,ES,ESE,ESGR,ESI,ESNT,ESPR,ESRT,ESS,ESTC,ETFC,ETN,ETR,ETRN,ETSY,EURN,EV,EVBG,EVH,EVOP,EVR,EVRG,EVRI,EVTC,EW,EWBC,EXAS,EXC,EXEL,EXLS,EXP,EXPD,EXPE,EXPO,EXR,EXTR,EYE,F,FAF,FANG,FARO,FAST,FATE,FB,FBC,FBHS,FBM,FBP,FCAU,FCF,FCFS,FCN,FCNCA,FCPT,FCX,FDP,FDS,FDX,FE,FELE,FEYE,FFBC,FFIN,FFIV,FG,FGEN,FHB,FHN,FIBK,FICO,FII,FIS,FISV,FIT,FITB,FIVE,FIVN,FIX,FIXX,FIZZ,FL,FLEX,FLIR,FLO,FLOW,FLR,FLS,FLT,FLWS,FLXN,FMBI,FMC,FN,FNB,FND,FNF,FNKO,FNV,FOCS,FOE,FOLD,FORM,FOXA,FOXF,FR,FRC,FRME,FRO,FRPT,FRT,FSCT,FSK,FSLR,FSLY,FSS,FTCH,FTDR,FTI,FTNT,FTS,FTV,FUL,FULT,FWONK,FWRD,G,GAB,GATX,GBCI,GBDC,GBT,GBX,GCP,GD,GDDY,GDI,GDOT,GDV,GE,GEF,GEO,GES,GGG,GH,GHC,GIB,GIII,GIL,GILD,GIS,GKOS,GL,GLIBA,GLNG,GLOB,GLPI,GLUU,GLW,GM,GMED,GMS,GNL,GNRC,GNTX,GNW,GO,GOLD"
Q1500_5="GOLF,GOOG,GOOGL,GOOS,GOSS,GPC,GPI,GPK,GPMT,GPN,GPS,GRA,GRMN,GRPN,GRUB,GS,GSHD,GSKY,GT,GTES,GTHX,GTLS,GTN,GVA,GWB,GWRE,GWW,H,HA,HAE,HAIN,HAL,HALO,HAS,HASI,HBAN,HBI,HBM,HCA,HCC,HCSG,HD,HDS,HE,HEES,HEI,HEI.A,HELE,HES,HFC,HGV,HHC,HI,HIG,HII,HIW,HL,HLF,HLI,HLNE,HLT,HLX,HMHC,HMN,HMSY,HNGR,HNI,HOG,HOLX,HOMB,HON,HOPE,HP,HPE,HPP,HPQ,HQY,HR,HRB,HRC,HRI,HRL,HRTX,HSC,HSIC,HST,HSY,HTA,HTGC,HTH,HTLD,HTZ,HUBB,HUBG,HUBS,HUM,HUN,HWC,HXL,HZNP,I,IAA,IAC,IAG,IART,IBKC,IBM,IBOC,IBP,IBTX,ICE,ICFI,ICLR,ICPT,ICUI,IDA,IDCC,IDXX,IEX,IFF,IGT,IIPR,IIVI,ILMN,ILPT,IMAX,IMMU,IMO,INCY,INDB,INFN,INFO,INGN,INGR,INN,INOV,INSM,INSP,INST,INSW,INT,INTC,INTU,INVA,INVH,INXN,IONS,IOSP,IOVA,IP,IPAR,IPG,IPGP,IPHI,IQV,IR,IRBT,IRDM,IRM,IRT,IRTC,IRWD,ISBC,ISRG,IT,ITCI,ITGR,ITRI,ITT,ITW,IVR,IVZ,J,JACK,JAG,JAZZ,JBGS,JBHT,JBL,JBLU,JBSS,JBT,JCI,JCOM,JEF,JELD,JHG,JJSF,JKHY,JLL,JNJ,JNPR,JPM,JPS,JRVR,JW.A,JWN,K,KAI,KALU"
Q1500_6="KAMN,KAR,KBH,KBR,KDP,KEM,KEX,KEY,KEYS,KFY,KGC,KHC,KIM,KKR,KL,KLAC,KLIC,KMB,KMI,KMPR,KMT,KMX,KN,KNL,KNSL,KNX,KO,KOD,KOS,KPTI,KR,KRA,KRC,KREF,KRG,KRNT,KRTX,KRYS,KSS,KSU,KTB,KTOS,KW,KWR,KYN,L,LAD,LADR,LAMR,LANC,LASR,LAUR,LAZ,LB,LBRDK,LBRT,LBTYA,LBTYK,LC,LCII,LDOS,LEA,LECO,LEG,LEN,LEVI,LFUS,LGF.A,LGIH,LGND,LH,LHCG,LHX,LII,LILAK,LIN,LITE,LIVN,LKQ,LLY,LM,LMT,LNC,LNG,LNN,LNT,LNTH,LOGI,LOGM,LOPE,LOW,LPG,LPLA,LPSN,LPT,LPX,LRCX,LRN,LSCC,LSI,LSTR,LSXMA,LSXMK,LTC,LTHM,LULU,LUV,LVGO,LVS,LW,LXP,LYB,LYFT,LYV,LZB,M,MA,MAA,MAC,MAG,MAIN,MAN,MANH,MANT,MAR,MAS,MASI,MAT,MATX,MAXR,MBUU,MC,MCD,MCHP,MCK,MCO,MCS,MCY,MD,MDB,MDC,MDGL,MDLA,MDLZ,MDP,MDRX,MDT,MDU,MED,MEDP,MEI,MELI,MEOH,MET,MFA,MFC,MGA,MGEE,MGLN,MGM,MGP,MGPI,MGRC,MGY,MHK,MHO,MIC,MIDD,MIK,MIME,MINI,MKC,MKL,MKSI,MKTX,MLAB,MLHR,MLI,MLM,MLNX,MMC,MMI,MMM,MMS,MMSI,MMYT,MNR,MNRL,MNRO,MNST,MNTA,MO,MODN,MOG.A,MOH,MORN,MOS,MPC,MPW,MPWR"
Q1500_7="MRC,MRCY,MRK,MRNA,MRO,MRTX,MRVL,MS,MSA,MSCI,MSFT,MSG,MSGN,MSI,MSM,MSTR,MTB,MTCH,MTD,MTDR,MTG,MTH,MTN,MTOR,MTRN,MTSC,MTSI,MTX,MTZ,MU,MUR,MUSA,MWA,MXIM,MXL,MYGN,MYL,MYOK,MYOV,NAD,NATI,NAV,NAVI,NBIX,NBL,NBR,NCLH,NCR,NDAQ,NDSN,NEA,NEE,NEM,NEO,NEOG,NEU,NEWR,NEX,NFG,NFLX,NG,NGHC,NGVT,NHI,NI,NJR,NKE,NKTR,NLOK,NLSN,NLY,NMFC,NMIH,NMRK,NMZ,NNN,NOC,NOG,NOMD,NOV,NOVT,NOW,NP,NPO,NRG,NRZ,NSA,NSC,NSIT,NSP,NSTG,NTAP,NTB,NTCT,NTGR,NTNX,NTR,NTRA,NTRS,NTUS,NUAN,NUE,NUS,NUVA,NVCR,NVDA,NVG,NVR,NVRO,NVST,NVT,NVTA,NWBI,NWE,NWL,NWN,NWSA,NXPI,NXRT,NXST,NYCB,NYMT,NYT,NZF,O,OAS,OC,ODFL,ODP,OEC,OFC,OFG,OFIX,OGE,OGS,OHI,OI,OII,OIS,OKE,OKTA,OLED,OLLI,OLN,OMC,OMCL,OMF,ON,ONB,ONTO,OPI,OPK,OR,ORA,ORCL,ORI,ORLY,OSB,OSIS,OSK,OSW,OTEX,OUT,OXM,OXY,OZK,PAAS,PACW,PAG,PAGS,PANW,PARR,PATK,PAYC,PAYX,PB,PBA,PBCT,PBF,PBH,PCAR,PCG,PCH,PCRX,PCTY,PD,PDCE,PDCO,PDM,PE,PEAK,PEB,PEG,PEGA,PEGI,PEN,PENN,PEP,PFE,PFG"
Q1500_8="PFGC,PFPT,PFSI,PG,PGR,PGRE,PGTI,PH,PHM,PHYS,PII,PINC,PINS,PJT,PK,PKG,PKI,PLAB,PLAN,PLAY,PLCE,PLD,PLMR,PLNT,PLT,PLUG,PLUS,PLXS,PM,PMT,PNC,PNFP,PNM,PNR,PNW,PODD,POL,POOL,POR,POST,POWI,PPBI,PPC,PPG,PPL,PRA,PRAA,PRAH,PRFT,PRGO,PRGS,PRI,PRLB,PRNB,PRO,PRSP,PRU,PS,PSA,PSB,PSEC,PSLV,PSMT,PSTG,PSX,PTC,PTCT,PTEN,PTLA,PTON,PUMP,PVG,PVH,PWR,PXD,PYPL,PZZA,QCOM,QDEL,QEP,QGEN,QLYS,QNST,QRTEA,QRVO,QSR,QTS,QTWO,QUOT,QURE,R,RACE,RAMP,RARE,RARX,RBA,RBC,RCI,RCII,RCL,RCM,RDFN,RDN,RDUS,RE,REAL,REG,REGI,REGN,RES,RETA,REXR,REZI,RF,RGA,RGEN,RGLD,RGNX,RGR,RH,RHI,RHP,RIG,RJF,RL,RLGY,RLI,RLJ,RMAX,RMBS,RMD,RNG,RNR,RNST,ROCK,ROG,ROIC,ROK,ROKU,ROL,ROLL,ROP,ROST,RP,RPAI,RPD,RPM,RPT,RRC,RRR,RS,RSG,RTN,RUN,RUSHA,RVLV,RWT,RXN,RY,RYN,S,SA,SABR,SAFM,SAGE,SAH,SAIA,SAIC,SAIL,SAM,SAND,SANM,SASR,SATS,SAVE,SBAC,SBCF,SBGI,SBH,SBLK,SBNY,SBRA,SBUX,SC,SCCO,SCHL,SCHW,SCI,SCL,SCS,SDC,SEAS,SEDG,SEE,SEIC,SEM,SERV,SF,SFIX,SFL"
Q1500_9="SFM,SFNC,SGEN,SGH,SGMO,SGMS,SHAK,SHEN,SHO,SHOO,SHOP,SHW,SIG,SIGI,SILK,SINA,SIRI,SITC,SITE,SIVB,SIX,SJI,SJM,SJR,SJW,SKT,SKX,SKY,SKYW,SLAB,SLB,SLF,SLG,SLGN,SLM,SM,SMAR,SMG,SMPL,SMTC,SNA,SNAP,SNBR,SNDR,SNPS,SNV,SNX,SO,SON,SONO,SPB,SPCE,SPG,SPGI,SPLK,SPOT,SPR,SPSC,SPWR,SPXC,SQ,SR,SRC,SRCI,SRCL,SRE,SRG,SRI,SRPT,SSB,SSD,SSNC,SSP,SSRM,SSTK,SSW,SSYS,ST,STAA,STAG,STAR,STAY,STE,STL,STLD,STMP,STNE,STNG,STOR,STRA,STT,STWD,STX,STZ,SU,SUI,SUM,SUPN,SVC,SVM,SVMK,SWAV,SWCH,SWI,SWK,SWKS,SWM,SWN,SWX,SXI,SXT,SYF,SYK,SYNA,SYNH,SYY,T,TALO,TAP,TARO,TBPH,TCBI,TCDA,TCF,TCMD,TCO,TD,TDC,TDG,TDOC,TDS,TDY,TEAM,TECD,TECH,TECK,TEL,TELL,TEN,TENB,TER,TERP,TEX,TFC,TFX,TGI,TGNA,TGT,TGTX,THC,THG,THO,THRM,THS,TIF,TILE,TIVO,TJX,TKR,TLRY,TMHC,TMO,TMUS,TNDM,TNET,TOL,TPH,TPL,TPR,TPRE,TPTX,TPX,TREE,TREX,TRGP,TRHC,TRI,TRIP,TRMB,TRMK,TRN,TRNO,TROW,TROX,TRP,TRQ,TRTN,TRTX,TRU,TRUP,TRV,TRWH,TSCO,TSE,TSEM,TSLA,TSLX,TSN,TTC,TTD"
Q1500_10="TTEK,TTMI,TTWO,TU,TVTY,TW,TWLO,TWNK,TWO,TWOU,TWTR,TXN,TXRH,TXT,TYG,TYL,UAA,UAL,UBER,UBS,UBSI,UCBI,UCTT,UDR,UE,UFPI,UFS,UGI,UHAL,UHS,UI,ULTA,UMBF,UMPQ,UNF,UNH,UNIT,UNM,UNP,UNVR,UPLD,UPS,UPWK,URBN,URI,USB,USFD,USM,USNA,USPH,UTHR,UTX,UVE,UVV,V,VAC,VAL,VAR,VBTX,VC,VCEL,VCYT,VEEV,VER,VET,VFC,VGR,VIAC,VIAV,VICI,VICR,VIRT,VLO,VLY,VMC,VMI,VMW,VNDA,VNE,VNO,VOYA,VREX,VRNS,VRNT,VRRM,VRSK,VRSN,VRTS,VRTU,VRTX,VSAT,VSH,VSLR,VST,VTR,VVI,VVV,VZ,W,WAB,WAFD,WAL,WAT,WBA,WBC,WBS,WBT,WCC,WCG,WCN,WD,WDAY,WDC,WDFC,WDR,WEC,WELL,WEN,WERN,WEX,WFC,WGO,WH,WHD,WHR,WING,WIX,WK,WLK,WLTW,WM,WMB,WMGI,WMS,WMT,WNC,WOR,WORK,WPC,WPM,WPX,WRB,WRE,WRI,WRK,WSBC,WSC,WSFS,WSM,WSO,WST,WTFC,WTI,WTM,WTR,WTS,WTTR,WU,WW,WWD,WWE,WWW,WY,WYND,WYNN,X,XEC,XEL,XENT,XHR,XLNX,XLRN,XNCR,XOM,XON,XPER,XPO,XRAY,XRX,XYL,Y,YELP,YETI,YEXT,YMAB,YNDX,YUM,YUMC,Z,ZAYO,ZBH,ZBRA,ZEN,ZG,ZGNX,ZION,ZIOP,ZM,ZNGA,ZS"
Q1500_11="ZTS,ZUMZ,ZUO,ZYME,SPY"

Q1500 = [Q1500_1, Q1500_2, Q1500_3, Q1500_4, Q1500_5, Q1500_6, Q1500_7, Q1500_8, Q1500_9, Q1500_10, Q1500_11]

# api key
al_headers = {
    "APCA-API-KEY-ID": "AKK40QK9V7AKT03XR9Q0", 
    "APCA-API-SECRET-KEY": "HAi59EidEC3LWU9MrqtCa35qtpli6fX50O4aMCDs"
}

def Merge(dict1, dict2): 
  res = {**dict1, **dict2} 
  return res 

# Retrieve and join data
def get_data(key,tickers,end):
  params={}
  df = pd.DataFrame()
  for e in range(len(end)):
    for t in range(len(tickers)):
      params['{}{}_{}'.format('al_params',t,e)] = {
          "symbols": tickers[t],
          "limit": str(data_limit),
          "end": end[e]
      }
      
      response = requests.get("https://data.alpaca.markets/v1/bars/1D", headers=key, params=params['{}{}_{}'.format('al_params',t,e)])
      results = response.json()
      df_i = pd.DataFrame.from_dict({(i,results[i][j]['t']): Merge(results[i][j], {'stock':i})
                                                            for i in results.keys() 
                                                            for j in range(len(results[i]))},
                                                           orient='index')
      df=pd.concat([df, df_i], axis=0)
  df=df.sort_values(['stock', 't']).reset_index(drop=True)
  df['t'] = pd.to_datetime(df['t'], unit='s').astype('datetime64[D]')
  df = df[['stock', 't', 'o', 'c', 'v']]
  
  del df_i, response
  gc.collect()
  return df

# Momentum-based RSI metrics 

# Set window for momentum/change period & number of periods to calculate RSI
#periods = [(2,6),(5,7),(10,8)]

def period_rsi(df, periods, *metrics):
  for metric in metrics:
    for day, ago in periods:
      df['{}{}'.format('rsi_mom',day)] = df[metric]/df.groupby('stock')[metric].shift(day)-1
      for a in range(ago+1):
        df['{}{}{}'.format('rsimom_pos', day, a)] = np.where(df.groupby('stock')['{}{}'.format('rsi_mom',day)].shift((day)*a)>=0, 
                                                            df.groupby('stock')['{}{}'.format('rsi_mom',day)].shift((day)*a), np.NaN)
        df['{}{}{}'.format('rsimom_neg', day, a)] = np.where(df.groupby('stock')['{}{}'.format('rsi_mom',day)].shift((day)*a)<=0, 
                                                            df.groupby('stock')['{}{}'.format('rsi_mom',day)].shift((day)*a), np.NaN)
      df['{}{}{}'.format('rsipos',day,ago)] = df.filter(regex=('{}{}{}'.format('rsimom_pos',day,'.*'))).mean(axis=1)
      df['{}{}{}'.format('rsineg',day,ago)] = -df.filter(regex=('{}{}{}'.format('rsimom_neg',day,'.*'))).mean(axis=1)
      df['{}{}{}'.format(metric, 'rsi_mom', day)] = np.where(df['{}{}{}'.format('rsineg',day,ago)].isna(), 100, 
                                                        np.where(df['{}{}{}'.format('rsipos',day,ago)].isna(), 0, 
                                                                  100 - 100/(1+df['{}{}{}'.format('rsipos',day,ago)]/df['{}{}{}'.format('rsineg',day,ago)])))
      df = df.drop(['{}{}{}'.format('rsipos',day,ago), '{}{}{}'.format('rsineg',day,ago)], axis=1)
      for a in range(ago+1):
        df = df.drop(['{}{}{}'.format('rsimom_pos', day, a), '{}{}{}'.format('rsimom_neg', day, a)], axis=1)
      df = df.drop(['{}{}'.format('rsi_mom',day)], axis=1)
  return df

# MACD Functions for Stock & SPY: calculates MACD metrics ((exp ma short/exp ma long)-1, macd, rsi of macd change)

# set windows for rsi function applied to MACD
#rsi_ranges = [6,18]
def macd(df, short, longg, signal, rsi_ranges, periods):
    df['{}{}{}{}'.format('macdd', signal, short, longg)]=(df.groupby('stock')['c'].rolling(short, win_type='triang').mean().reset_index(level=0, drop=True) \
    /df.groupby('stock')['c'].rolling(longg, win_type='triang').mean().reset_index(level=0, drop=True)) - 1
    
    df['{}{}{}{}'.format('macd', signal, short, longg)]= (df.groupby('stock')['c'].rolling(short, win_type='triang').mean().reset_index(level=0, drop=True) \
    - df.groupby('stock')['c'].rolling(longg, win_type='triang').mean().reset_index(level=0, drop=True))

    df['{}{}{}{}'.format('macd', signal, short, longg)]=df['{}{}{}{}'.format('macd', signal, short, longg)] \
    - df.groupby('stock')['{}{}{}{}'.format('macd', signal, short, longg)].rolling(signal, win_type='triang').mean().reset_index(level=0, drop=True)

    df['{}{}{}{}{}'.format('macd', signal, short, longg, 'ch')] = df.groupby('stock')['{}{}{}{}'.format('macd', signal, short, longg)].diff()
    #df=consec(df, '{}{}{}{}'.format('macd', signal, short, longg), '{}{}{}{}{}'.format('macd', signal, short, longg, 'ch'))
    #df=metric_rsi(df, ['{}{}{}{}{}'.format('macd', signal, short, longg, 'ch')], rsi_ranges #[5,11,21,32,42,53]
    #              )
    df=period_rsi(df, periods, '{}{}{}{}'.format('macd', signal, short, longg))

    df=df.drop(['{}{}{}{}{}'.format('macd', signal, short, longg, 'ch')], axis=1)

    return df

def spymacd(df, short, longg, signal, rsi_ranges, periods):
    df['{}{}{}{}'.format('spymacdd', signal, short, longg)]=(df.groupby('stock')['spy_c'].rolling(short, win_type='triang').mean().reset_index(level=0, drop=True) \
    /df.groupby('stock')['spy_c'].rolling(longg, win_type='triang').mean().reset_index(level=0, drop=True)) - 1
    
    df['{}{}{}{}'.format('spymacd', signal, short, longg)]= (df.groupby('stock')['spy_c'].rolling(short, win_type='triang').mean().reset_index(level=0, drop=True) \
    - df.groupby('stock')['spy_c'].rolling(longg, win_type='triang').mean().reset_index(level=0, drop=True))

    df['{}{}{}{}'.format('spymacd', signal, short, longg)]=df['{}{}{}{}'.format('spymacd', signal, short, longg)] \
    - df.groupby('stock')['{}{}{}{}'.format('spymacd', signal, short, longg)].rolling(signal, win_type='triang').mean().reset_index(level=0, drop=True) 
    
    df['{}{}{}{}{}'.format('spymacd', signal, short, longg, 'ch')] = df.groupby('stock')['{}{}{}{}'.format('spymacd', signal, short, longg)].diff()
    #df=consec(df, '{}{}{}{}'.format('spymacd', signal, short, longg), '{}{}{}{}{}'.format('spymacd', signal, short, longg, 'ch'))
    #df=metric_rsi(df, ['{}{}{}{}{}'.format('spymacd', signal, short, longg, 'ch')], rsi_ranges #[5,11,21,32,42,53]
    #              )
    df=period_rsi(df, periods, '{}{}{}{}'.format('spymacd', signal, short, longg))

    df=df.drop(['{}{}{}{}{}'.format('spymacd', signal, short, longg, 'ch')], axis=1)

    return df

# Drawdown Functions: days from max/min drawdown & difference from current price over window 'days'
def drawdown(df, *days):
  df['rollmax'] = df.groupby('stock')['c'].cummax().reset_index(level=0, drop=True)
  df['counter'] = np.where(df['rollmax']==df['c'],0,1)
  df['swings'] = df.groupby('stock')['counter'].cumsum()
  df['rollmax_dddays'] = df.groupby(['stock','swings']).cumcount()-1
  #df['rollmax'] = df['c']/df['rollmax']-1 
  df['rollmax'] = df['c'] - df['rollmax']

  df['rollmin'] = df.groupby('stock')['c'].cummin().reset_index(level=0, drop=True)
  df['counter'] = np.where(df['rollmin']==df['c'],0,1)
  df['swings'] = df.groupby('stock')['counter'].cumsum()
  df['rollmin_dddays'] = df.groupby(['stock','swings']).cumcount()-1
  #df['rollmin'] = df['c']/df['rollmin']-1 
  df['rollmin'] = df['c'] - df['rollmin']

  df = df.drop(['counter','swings'], axis=1)

  for day in days:
    df['{}{}'.format('rollmax',day)] = df.groupby('stock')['c'].rolling(day).max().reset_index(level=0, drop=True)
    df['counter'] = np.where(df['{}{}'.format('rollmax',day)]==df['c'],0,1)
    df['swings'] = df.groupby('stock')['counter'].cumsum()
    df['{}{}'.format('rollmax_dddays', day)] = df.groupby(['stock','swings']).cumcount()-1
    #df['{}{}'.format('rollmax',day)] = df['c']/df['{}{}'.format('rollmax',day)]-1 
    df['{}{}'.format('rollmax',day)] = df['c'] - df['{}{}'.format('rollmax',day)]

    df['{}{}'.format('rollmin',day)] = df.groupby('stock')['c'].rolling(day).min().reset_index(level=0, drop=True)
    df['counter'] = np.where(df['{}{}'.format('rollmin',day)]==df['c'],0,1)
    df['swings'] = df.groupby('stock')['counter'].cumsum()
    df['{}{}'.format('rollmin_dddays', day)] = df.groupby(['stock','swings']).cumcount()-1
    #df['{}{}'.format('rollmin',day)] = df['c']/df['{}{}'.format('rollmin',day)]-1 
    df['{}{}'.format('rollmin',day)] = df['c'] - df['{}{}'.format('rollmin',day)]

    df = df.drop(['counter','swings'], axis=1)
  return df

# Oscillator Function: oscilaitor calculation over long & short anchor windows 'days'
def oscillator(df, *days):
  for (k,d) in days:
    df['{}{}'.format('osc', k)]=100*(df['c']-df.groupby('stock')['c'].rolling(k).min().reset_index(level=0, drop=True)) \
    /(df.groupby('stock')['c'].rolling(k).max().reset_index(level=0, drop=True)-df.groupby('stock')['c'].rolling(k).min().reset_index(level=0, drop=True))

    df['{}{}{}{}'.format('osc', k, d, 'sma')]=df.groupby('stock')['{}{}'.format('osc', k)].rolling(d).mean().reset_index(level=0, drop=True)
    df['{}{}{}{}'.format('osc',k,d,'diff')]=df['{}{}'.format('osc', k)]-df['{}{}{}{}'.format('osc', k, d, 'sma')]
    df = df.drop(['{}{}'.format('osc', k), '{}{}{}{}'.format('osc', k, d, 'sma')], axis=1)
  return df

# Correlation Function for Beta Calculations
def correl(x, day):
    return pd.DataFrame(x['daily_return'].rolling(day).corr(x['spy_return']))

# Beta Calculation

def beta(df, beta_days):
  for day in beta_days: #13, 21, 34, 55
    df['{}{}'.format(day,'day_corr')] = df.groupby('stock')[['daily_return','spy_return']].apply(correl, day).shift(day).reset_index(level=0, drop=True)
    df['{}{}'.format(day,'day_stdcalc')] = df.groupby('stock')['daily_return'].rolling(day).std().shift(day).reset_index(level=0, drop=True) \
    /df.groupby('stock')['spy_return'].rolling(day).std().shift(day).reset_index(level=0, drop=True)
    df['{}{}'.format('beta_',day)] = df['{}{}'.format(day,'day_stdcalc')] * df['{}{}'.format(day,'day_corr')] 
    df=df.drop(['{}{}'.format(day,'day_corr'),'{}{}'.format(day,'day_stdcalc')], axis=1)
  return df

# Daily Change Function in percent 
def daily_change(df, **kwargs):
  for key, value in kwargs.items():
    #df[key]=df.groupby('stock')[value].pct_change()
    df[key]=df[value] - df.groupby('stock')[value].shift(1)
  return df

# Date-range metrics
def main(df, day_ranges, periods, rsi_ranges):

# Next day's open & closing price
  df['cplus1'] = df.groupby('stock')['c'].shift(-1)
  df['oplus1'] = df.groupby('stock')['o'].shift(-1)

# Daily closing & volume change 
  df=daily_change(df, daily_return='c', daily_volch='v')


# Isolate SPY
  spy_df = df[(df['stock'] == 'SPY')][['daily_return', 't', 'c']]
  spy_df=spy_df.rename(columns = {'daily_return':'spy_return', 'c':'spy_c'})
  df=pd.merge(df, spy_df, on='t')
  df = df[(df['stock'] != 'SPY')].sort_values(['stock', 't']).reset_index(drop=True) 

# On-Balance Volume Calculation
  #df['pvol'] = df['c']*df['v']
  df['volsign'] = np.where(df['daily_return']>0, df['v'], -df['v'])
  df['obv'] = df.groupby('stock')['volsign'].cumsum()
  df['obvch'] = df['obv']/df.groupby('stock')['obv'].shift(1)-1
  df = df.drop(['volsign'#, 'obv'
                ], axis=1) 

# Price x On-Balance Volume
  df['pobv'] = df['c']*df['obv']
  #df['pvolch'] = df['pvol']/df.groupby('stock')['pvol'].shift(1)-1
  df['pobvch'] = df['pobv']/df.groupby('stock')['pobv'].shift(1)-1

# metrics created for each window below for each stock and for SPY: momentum, moving avg deviation, standard deviation (volatility), change in mad, 
  for day in day_ranges:
    df['{}{}'.format('momentum',day)] = df['c']/df.groupby('stock')['c'].shift(day)-1
    #df['{}{}'.format('obv_mom',day)] = df['obv']/df.groupby('stock')['obv'].shift(day)-1
    df['{}{}'.format('pobv_mom',day)] = df['pobv']/df.groupby('stock')['pobv'].shift(day)-1
    #df['{}{}'.format('volma',day)] = df['v'] - df.groupby('stock')['v'].rolling(day).mean().reset_index(level=0, drop=True)
    #df['{}{}'.format('volma',day)] = df['v'] - df.groupby('stock')['v'].rolling(day).mean().reset_index(level=0, drop=True)
    df['{}{}'.format('obvma',day)] = df['obv'] - df.groupby('stock')['obv'].rolling(day).mean().reset_index(level=0, drop=True)
    #df['{}{}'.format('pvolma',day)] = df['pvol'] - df.groupby('stock')['pvol'].rolling(day).mean().reset_index(level=0, drop=True)
    df['{}{}'.format('pobvma',day)] = df['pobv'] - df.groupby('stock')['pobv'].rolling(day).mean().reset_index(level=0, drop=True)
    #df['{}{}'.format('mad',day)] = df['c']/df.groupby('stock')['c'].rolling(day).mean().reset_index(level=0, drop=True) - 1
    df['{}{}'.format('mad',day)] = df['c'] - df.groupby('stock')['c'].rolling(day).mean().reset_index(level=0, drop=True)
    #df['{}{}'.format('bolltop_',day)] = df['c'] /(df['{}{}'.format('mad_',day)] - 2*df.groupby('stock')['c'].rolling(day).std().reset_index(level=0, drop=True)) - 1
    df['{}{}'.format('std',day)] = df.groupby('stock')['c'].rolling(day).std().reset_index(level=0, drop=True)/df['c']
    df['{}{}'.format('pobv_std',day)] = df.groupby('stock')['pobv'].rolling(day).std().reset_index(level=0, drop=True)/df['pobv']

    df['{}{}'.format('spymomentum',day)] = df['spy_c']/df.groupby('stock')['spy_c'].shift(day)-1
    #df['{}{}'.format('spymad',day)] = df['spy_c']/df.groupby('stock')['spy_c'].rolling(day).mean().reset_index(level=0, drop=True) - 1
    df['{}{}'.format('spymad',day)] = df['spy_c'] - df.groupby('stock')['spy_c'].rolling(day).mean().reset_index(level=0, drop=True)
    #df['{}{}'.format('bolltop_',day)] = df['c'] /(df['{}{}'.format('mad_',day)] - 2*df.groupby('stock')['c'].rolling(day).std().reset_index(level=0, drop=True)) - 1
    df['{}{}'.format('spystd',day)] = df.groupby('stock')['spy_c'].rolling(day).std().reset_index(level=0, drop=True)/df['spy_c']
    
    df['{}{}'.format('madch',day)] = df.groupby('stock')['{}{}'.format('mad', day)].diff()
    df['{}{}'.format('spymadch',day)] = df.groupby('stock')['{}{}'.format('spymad', day)].diff()
    df['{}{}'.format('obvmach',day)] = df.groupby('stock')['{}{}'.format('obvma', day)].diff()
    df['{}{}'.format('pobvmach',day)] = df.groupby('stock')['{}{}'.format('pobvma', day)].diff()

  for day in day_ranges_slim:
    df=period_rsi(df, periods, '{}{}'.format('obvma',day), '{}{}'.format('pobvma',day), '{}{}'.format('mad',day), '{}{}'.format('spymad',day),
                  '{}{}'.format('momentum',day), '{}{}'.format('pobv_mom',day), '{}{}'.format('spymomentum',day), '{}{}'.format('madch',day), '{}{}'.format('spymadch',day),  
                  '{}{}'.format('obvmach',day), 
                  '{}{}'.format('pobvmach',day),
                  '{}{}'.format('pobv_std',day)
                  )
    
  return df

##Download Data and Create Metrics

df=get_data(al_headers, Q1500, [end_date, ending
                                ])

# set day windows for day_ranges below for metric calculations
day_ranges = [3,7,15,31] #3,5,11,21,42
day_ranges_slim = [3,7,15] #3,5,11,21,42
rsi_ranges = [14]
periods = [(2,10),(5,8),(10,6)
           ]
# can optionally re-set windows for rsi calculations later on

df=main(df, day_ranges, periods, rsi_ranges)

# set windows for beta function
beta_days = [11,50]
df=beta(df, beta_days)

# Additional Metrics from functions defined above
df=oscillator(df, (13,2), #(11,3), 
           (35,6), 
           #(42,7), 
           #(84,10)
           (65,10)
           ) 
df=drawdown(df, 14,45,84#,84
         )
df=macd(df, 7, 11, 21, rsi_ranges, periods)
df=macd(df, 21, 28, 56, rsi_ranges, periods)
#macd(df, 38, 50, 100)
df=spymacd(df, 7, 11, 21, rsi_ranges, periods)
df=spymacd(df, 21, 28, 56, rsi_ranges, periods)
#spymacd(df, 38, 50, 100)

df=period_rsi(df, periods, 'c', 'obv', 'pobv'#, 'pvol'
              )

# Create metrics of existing metrics from b1 & b2 days ago

#df.columns.tolist()
b1=6
b2=25
backago=df.drop(['stock', 't', 'o', 'c', 'v', 'cplus1','oplus1',#'pvol',
                 'daily_return', 'daily_volch', 'spy_return', 'spy_c'], axis=1).columns.tolist()
for metric in backago:
  df['{}{}{}'.format(metric, '_b', b1)]=df.groupby('stock', observed=True)[metric].shift(b1).reset_index(drop=True).astype('float32')
  df['{}{}{}'.format(metric, '_b', b2)]=df.groupby('stock', observed=True)[metric].shift(b2).reset_index(drop=True).astype('float32')

len(df.columns)

##Additional non-price & volume based Features

# Bitcoin & VIX data
bitcoin_data = pd.read_csv("https://data.bitcoinity.org/export_data.csv?currency=USD&data_type=price_volume&r=day&t=lb&timespan=all&vu=curr", header=0)
bitcoin_data.columns=['date', 'price', 'volume']
bitcoin_data['date'] = bitcoin_data['date'].str[:10].astype('datetime64[D]')
spy_sector = pd.read_csv("https://raw.githubusercontent.com/wangjason11/sandbox/master/retire/stocks_sectors3.csv")
vix = pd.read_csv("http://www.cboe.com/publish/scheduledtask/mktdata/datahouse/vixcurrent.csv", skiprows=[0])
vix.columns=['date', 'vix_open', 'vix_high', 'vix_low', 'vix_close']
vix['date'] = vix['date'].astype('datetime64[D]')

# REMEMBER TO CHANGE BTC END DATE WHEN MAKING PREDICTIONS
bitcoin_end = ending_date #datetime.today() - timedelta(hours=24) 
bitcoin_end = bitcoin_end.strftime('%Y%m%d')

# Add BTC data
bdf=bitcoin_data[bitcoin_data['date'].isin(pd.date_range(start='20141001', end=bitcoin_end, freq='B').strftime('%Y%m%d'))].reset_index(drop=True)
bdf['cap'] = bdf['price']*bdf['volume']
bdf = bdf[['date', 'cap']]

for day in day_ranges:
    bdf['{}{}'.format('btcch', day)] = bdf['cap']/bdf['cap'].shift(day)-1

#print(bdf.iloc[0:50,:])
bdf=bdf.rename(columns = {'date':'btc_date'})
dff=pd.merge(df, bdf, left_on='t', right_on='btc_date').reset_index(drop=True)

# Add Sector data
spy_sector = spy_sector[['symbol', 'sector']]
spy_sector['sector'] = spy_sector['sector'].astype(str)

dff=pd.merge(dff, spy_sector, left_on='stock', right_on='symbol').reset_index(drop=True)

# Add VIX data
vix = vix[['date', 'vix_close']]

for day in day_ranges:
    vix['{}{}'.format('vixch', day)] = vix['vix_close']/vix['vix_close'].shift(day)-1

vix=vix.rename(columns = {'date':'vix_date'})
dff=pd.merge(dff, vix, left_on='t', right_on='vix_date').sort_values(['stock', 't']).reset_index(drop=True)

#dff.describe()

del bitcoin_data, spy_sector, vix, bdf, df
gc.collect()

# Binarize sector data
for col in ['sector']:
    dummies = pd.get_dummies(dff[col], prefix=col)
    dff = pd.concat([dff, dummies], axis=1)
    dff.drop(col, axis=1, inplace=True)

# convert numeric data type for faster processing

cols = dff.select_dtypes(exclude=['object', 'datetime', 'uint8', 'int64', 'float32']).columns

dff[cols] = dff[cols].astype('float32')

##Create Prediction Variable

dff['nextd_return']=dff.groupby('stock')['cplus1'].shift(-2)/dff['cplus1']-1
dff['buy']=np.where(dff['nextd_return']>0, 1, 0)
#dff['nextd_return']=dff['nextd_return'].replace([np.inf, -np.inf, np.nan], -1.1)

# Input ratio for negative sell curve 
curve_ratio=0.5
dff['buy']=0
dff['buy_expand']=0

# Set curve window day ranges 
buy_windows = [#5,
               7,8,9,10,11]
for day in buy_windows:
  if day==5:
    dff['{}{}{}'.format('buy_',day,'d')] = np.where(dff.groupby('stock')['cplus1'].rolling(day).max().shift(-day+1).reset_index(level=0, drop=True)/dff['oplus1']-1>=0.05, 1, 0)
    dff['{}{}{}'.format('sell_',day,'d')] = np.where(dff.groupby('stock')['cplus1'].rolling(day).min().shift(-day+1).reset_index(level=0, drop=True)/dff['oplus1']-1<=-0.05*curve_ratio, 1, 0)
  else:
    dff['{}{}{}'.format('buy_',day,'d')] = np.where(dff.groupby('stock')['cplus1'].rolling(day).max().shift(-day+1).reset_index(level=0, drop=True)/dff['oplus1']-1>=(day-1)/100, 1, 0)
    dff['{}{}{}'.format('sell_',day,'d')] = np.where(dff.groupby('stock')['cplus1'].rolling(day).min().shift(-day+1).reset_index(level=0, drop=True)/dff['oplus1']-1<=-(day-1)/100*curve_ratio, 1, 0)

  dff['{}{}'.format('frollmax',day)] = dff.groupby('stock')['cplus1'].rolling(day).max().shift(-day+1).reset_index(level=0, drop=True)
  dff['counter'] = np.where(dff['{}{}'.format('frollmax',day)]==dff.groupby('stock')['cplus1'].shift(-day+1),0,1)
  dff['swings'] = dff.groupby('stock')['counter'].cumsum()
  dff['{}{}'.format('frollmax_dddays', day)] = dff.groupby(['stock','swings']).cumcount(ascending=False)-1
  #dff=dff.drop(['{}{}'.format('frollmax',day), 'counter', 'swings'], axis=1)

  dff['{}{}'.format('frollmin',day)] = dff.groupby('stock')['cplus1'].rolling(day).min().shift(-day+1).reset_index(level=0, drop=True)
  dff['counter'] = np.where(dff['{}{}'.format('frollmin',day)]==dff.groupby('stock')['cplus1'].shift(-day+1),0,1)
  dff['swings'] = dff.groupby('stock')['counter'].cumsum()
  dff['{}{}'.format('frollmin_dddays', day)] = dff.groupby(['stock','swings']).cumcount(ascending=False)-1
  #dff=dff.drop(['{}{}'.format('frollmin',day), 'counter', 'swings'], axis=1)
  if day==buy_windows[0]:
    dff['buy']=dff['buy']+np.where(dff['{}{}{}'.format('buy_',day,'d')]==1, 
                                   np.where((dff['{}{}{}'.format('sell_',day,'d')]==1) & (dff['{}{}'.format('frollmax_dddays', day)]>dff['{}{}'.format('frollmin_dddays', day)]),-1,1),
                                   np.where(dff['{}{}{}'.format('sell_',day,'d')]==1,-1,0))
  else:
    dff['buy']=np.where((dff['buy']==-1) | (dff['buy']>0), dff['buy'],
                        dff['buy']+np.where(dff['{}{}{}'.format('buy_',day,'d')]==1, 
                                   np.where((dff['{}{}{}'.format('sell_',day,'d')]==1) & (dff['{}{}'.format('frollmax_dddays', day)]>dff['{}{}'.format('frollmin_dddays', day)]),-1,1),
                                   np.where(dff['{}{}{}'.format('sell_',day,'d')]==1,-1,0))
                        )
    
  dff['buy_expand']=dff['buy_expand']+dff['{}{}{}'.format('buy_',day,'d')]
  dff=dff.drop(['{}{}'.format('frollmax',day), '{}{}'.format('frollmin',day), '{}{}'.format('frollmax_dddays', day), '{}{}'.format('frollmin_dddays', day), 'counter', 'swings'], axis=1)

dff['buy']=np.where(dff['buy']>0,1,0)
dff['buy_expand']=np.where(dff['buy_expand']>0,1,0)

# Input ratio for negative sell curve 
curve_ratio=0.5
dff['buy']=0
dff['buy_expand']=0

# Set curve window day ranges 
buy_windows = [3,4,5]
for day in buy_windows:
  dff['{}{}{}'.format('buy_',day,'d')] = np.where(dff.groupby('stock')['cplus1'].rolling(day).max().shift(-day+1).reset_index(level=0, drop=True)/dff['oplus1']-1>=math.floor((day+1)/100 * 100)/100.0, 1, 0)
  dff['{}{}{}'.format('sell_',day,'d')] = np.where(dff.groupby('stock')['cplus1'].rolling(day).min().shift(-day+1).reset_index(level=0, drop=True)/dff['oplus1']-1<=-(day*+1)/100*curve_ratio, 1, 0)

  dff['{}{}'.format('frollmax',day)] = dff.groupby('stock')['cplus1'].rolling(day).max().shift(-day+1).reset_index(level=0, drop=True)
  dff['counter'] = np.where(dff['{}{}'.format('frollmax',day)]==dff.groupby('stock')['cplus1'].shift(-day+1),0,1)
  dff['swings'] = dff.groupby('stock')['counter'].cumsum()
  dff['{}{}'.format('frollmax_dddays', day)] = dff.groupby(['stock','swings']).cumcount(ascending=False)-1
  #dff=dff.drop(['{}{}'.format('frollmax',day), 'counter', 'swings'], axis=1)

  dff['{}{}'.format('frollmin',day)] = dff.groupby('stock')['cplus1'].rolling(day).min().shift(-day+1).reset_index(level=0, drop=True)
  dff['counter'] = np.where(dff['{}{}'.format('frollmin',day)]==dff.groupby('stock')['cplus1'].shift(-day+1),0,1)
  dff['swings'] = dff.groupby('stock')['counter'].cumsum()
  dff['{}{}'.format('frollmin_dddays', day)] = dff.groupby(['stock','swings']).cumcount(ascending=False)-1
  #dff=dff.drop(['{}{}'.format('frollmin',day), 'counter', 'swings'], axis=1)
  if day==buy_windows[0]:
    dff['buy']=dff['buy']+np.where(dff['{}{}{}'.format('buy_',day,'d')]==1, 
                                   np.where((dff['{}{}{}'.format('sell_',day,'d')]==1) & (dff['{}{}'.format('frollmax_dddays', day)]>dff['{}{}'.format('frollmin_dddays', day)]),-1,1),
                                   np.where(dff['{}{}{}'.format('sell_',day,'d')]==1,-1,0))
  else:
    dff['buy']=np.where((dff['buy']==-1) | (dff['buy']>0), dff['buy'],
                        dff['buy']+np.where(dff['{}{}{}'.format('buy_',day,'d')]==1, 
                                   np.where((dff['{}{}{}'.format('sell_',day,'d')]==1) & (dff['{}{}'.format('frollmax_dddays', day)]>dff['{}{}'.format('frollmin_dddays', day)]),-1,1),
                                   np.where(dff['{}{}{}'.format('sell_',day,'d')]==1,-1,0))
                        )
    
  dff['buy_expand']=dff['buy_expand']+dff['{}{}{}'.format('buy_',day,'d')]
  dff=dff.drop(['{}{}'.format('frollmax',day), '{}{}'.format('frollmin',day), '{}{}'.format('frollmax_dddays', day), '{}{}'.format('frollmin_dddays', day), 'counter', 'swings'], axis=1)

dff['buy']=np.where(dff['buy']>0,1,0)
dff['buy_expand']=np.where(dff['buy_expand']>0,1,0)

# Alternative return output for regression instead

# Set target days window for max/min return calculation (also equivalent to max day to hold stock)
buy_windows = [5]
for day in buy_windows:
  dff['{}{}{}'.format('buy_',day,'d')] = dff.groupby('stock')['cplus1'].rolling(day).max().shift(-day+1).reset_index(level=0, drop=True)/dff['oplus1']-1
  dff['{}{}{}'.format('sell_',day,'d')] = dff.groupby('stock')['cplus1'].rolling(day).min().shift(-day+1).reset_index(level=0, drop=True)/dff['oplus1']-1

  dff['{}{}'.format('frollmax',day)] = dff.groupby('stock')['cplus1'].rolling(day).max().shift(-day+1).reset_index(level=0, drop=True)
  dff['counter'] = np.where(dff['{}{}'.format('frollmax',day)]==dff.groupby('stock')['cplus1'].shift(-day+1),0,1)
  dff['swings'] = dff.groupby('stock')['counter'].cumsum()
  dff['{}{}'.format('frollmax_dddays', day)] = dff.groupby(['stock','swings']).cumcount(ascending=False)-1
  dff=dff.drop(['counter', 'swings'], axis=1)

  dff['{}{}'.format('frollmin',day)] = dff.groupby('stock')['cplus1'].rolling(day).min().shift(-day+1).reset_index(level=0, drop=True)
  dff['counter'] = np.where(dff['{}{}'.format('frollmin',day)]==dff.groupby('stock')['cplus1'].shift(-day+1),0,1)
  dff['swings'] = dff.groupby('stock')['counter'].cumsum()
  dff['{}{}'.format('frollmin_dddays', day)] = dff.groupby(['stock','swings']).cumcount(ascending=False)-1
  dff=dff.drop(['counter', 'swings'], axis=1)

  #dff=dff.drop(['{}{}'.format('frollmin',day), 'counter', 'swings'], axis=1)
  dff['buy']=np.where(dff['{}{}'.format('frollmax_dddays', day)]>dff['{}{}'.format('frollmin_dddays', day)], dff['{}{}{}'.format('sell_',day,'d')], 
                      dff['{}{}{}'.format('buy_',day,'d')])
  dff=dff.drop(['{}{}'.format('frollmax',day), '{}{}'.format('frollmin',day), '{}{}'.format('frollmax_dddays', day), '{}{}'.format('frollmin_dddays', day)#, 
                #'counter', 'swings'#, '{}{}{}'.format('buy_',day,'d'), '{}{}{}'.format('sell_',day,'d')
                ], axis=1)

dff['buy11+'] = np.where((dff['buy_11d']==1) & (dff['buy_16d']==1) & (dff['buy_21d']==1) & (dff['buy_32d']==1) & (dff['buy_42d']==1), 1, 0)
dff['buy11-21'] = np.where((dff['buy_11d']==1) & (dff['buy_16d']==1) & (dff['buy_21d']==1), 1, 0)
dff['buy11-32'] = np.where((dff['buy_11d']==1) & (dff['buy_16d']==1) & (dff['buy_21d']==1)  & (dff['buy_32d']==1), 1, 0)
dff['buy11&32'] = np.where((dff['buy_11d']==1) & (dff['buy_32d']==1), 1, 0)
dff['buy11&42'] = np.where((dff['buy_11d']==1) & (dff['buy_42d']==1), 1, 0)

dff['buy16+'] = np.where((dff['buy_16d']==1) & (dff['buy_21d']==1) & (dff['buy_32d']==1) & (dff['buy_42d']==1), 1, 0)
dff['buy16-32'] = np.where((dff['buy_16d']==1) & (dff['buy_21d']==1) & (dff['buy_32d']==1), 1, 0)
dff['buy16&32'] = np.where((dff['buy_16d']==1) & (dff['buy_32d']==1), 1, 0)
dff['buy16&42'] = np.where((dff['buy_16d']==1) & (dff['buy_42d']==1), 1, 0)

dff['buy21+'] = np.where((dff['buy_21d']==1) & (dff['buy_32d']==1) & (dff['buy_42d']==1), 1, 0)
dff['buy21-32'] = np.where((dff['buy_21d']==1) & (dff['buy_32d']==1), 1, 0)
dff['buy21&42'] = np.where((dff['buy_21d']==1) & (dff['buy_42d']==1), 1, 0)
dff['buy32+'] = np.where((dff['buy_32d']==1) & (dff['buy_42d']==1), 1, 0)

dff.filter(regex=('{}{}'.format('buy','.*'))).describe()


# Drop non-features

dff=dff.drop([#'c', 'v', 
              'cplus1', 'spy_c', #'spy_return', 
              'vix_date', 'btc_date', 'symbol', 'oplus1', 'o', #'obvch',
              'cap', 'vix_close'#, 'sector'
             ], axis=1)

# Remove NULLs

dff['nextd_return']=dff['nextd_return'].replace([np.inf, -np.inf, np.nan], -1.1)

dff=dff.replace([np.inf, -np.inf], np.nan)
#for to_replace in [np.inf, -np.inf]:
#    dff = dff.mask(dff == to_replace, np.nan) 
dff=dff.dropna()

# Create date for cut-off between training & prediction data

#cutoff = datetime.today() - timedelta(days=123) 
cutoff = ending_date - timedelta(days=13) # can be set to other values longer or shorter
predict=dff[dff.t > cutoff.strftime('%Y-%m-%d')]
dff=dff[dff.t <= cutoff.strftime('%Y-%m-%d')]

#dff = pd.read_csv(io.StringIO(uploaded['dff_data.csv'].decode('utf-8')), infer_datetime_format=True)
stock_data=dff[['stock', 't']]
y = dff['buy']
yreturn = dff['nextd_return']

# Drop additional non-features for training data

X = dff.drop([#'buy_5d', 'buy_7d', 'buy_8d', 'buy_9d', 'buy_10d', 'buy_11d', #'buy_21d', 
              'buy', 'stock', 't', 'nextd_return'#, 'buyall', 'buy8'
              ], axis=1)


features = X.columns.tolist()
#X.describe()
len(features)


# Parameters for Grid-search & parameter optimization 

#mms = MinMaxScaler()
sc = StandardScaler()
#X_train = mms.fit_transform(X_train)
#X_test = mms.transform(X_test)

trees = ExtraTreesClassifier(oob_score=True, n_jobs=-1, random_state=165, bootstrap=True, class_weight='balanced'
                             )
treesr = ExtraTreesRegressor(oob_score=True, n_jobs=-1, random_state=165, bootstrap=True#, class_weight='balanced'
                             )
#nb = BernoulliNB() 
#gnb = GaussianNB()
#gnb = GaussianNB(priors=np.array([0.035, 0.965]))
#lgm = LogisticRegression(max_iter=5000)
pca=IncrementalPCA()
#kpca=KernelPCA(kernel='rbf', n_jobs=-1)
#svc_model = SVC(kernel='rbf', max_iter=-1, random_state=0)
#nn = MLPClassifier(hidden_layer_sizes=[300,100,50], activation='relu', solver='adam')
#nn = MLPClassifier(activation='relu', solver='adam', random_state=500)
#br = BinaryRelevance(#require_dense=[True, True]
#)

cachedir = mkdtemp()
#pipe = skpipeline.Pipeline([('scaling', mms), ('feature_select', 'passthrough'), ('estimator', svc_model)], memory=cachedir)
#pipe = skpipeline.Pipeline([('scaling', mms), ('feature_select', 'passthrough'), ('estimator', nb)], memory=cachedir)
#pipe = skpipeline.Pipeline([('scaling', mms), ('feature_select', pca), ('estimator', OneVsRestClassifier(nb, n_jobs=-1))], memory=cachedir)
pipe3 = skpipeline.Pipeline([('scaling', sc), ('feature_select', pca), ('estimator', trees)], memory=cachedir)
pipe4 = skpipeline.Pipeline([('scaling', sc), ('feature_select', pca), ('estimator', treesr)], memory=cachedir)
#pipe = skpipeline.Pipeline([('scaling', mms), ('feature_select', kpca), ('estimator', gnb)], memory=cachedir)
#pipe = skpipeline.Pipeline([('scaling', mms), ('estimator', trees)], memory=cachedir)
pipe = skpipeline.Pipeline([('estimator', trees)], memory=cachedir)
pipe2 = skpipeline.Pipeline([('scaling', sc), ('estimator', pca)], memory=cachedir)
#pipe = skpipeline.Pipeline([('scaling', mms), ('feature_select', pca), ('estimator', nb)], memory=cachedir)
#pipe = skpipeline.Pipeline([('scaling', mms), ('feature_select', pca), ('estimator', svc_model)], memory=cachedir)
#pipe = skpipeline.Pipeline([('scaling', mms), ('feature_select', pca), ('estimator', nn)], memory=cachedir)
#pipe = skpipeline.Pipeline([('scaling', mms), ('feature_select', pca), ('estimator', br)], memory=cachedir)

pca110=IncrementalPCA(n_components=110)
xgb_model = xgb.XGBClassifier(#n_estimators=300, 
                              objective='binary:logistic', booster='gbtree', n_jobs=-1, #subsample=0.8, 
                              #colsample_bytree=0.8, 
                              colsample_bylevel=0.8, colsample_bynode=0.8, 
                              random_state=165)
pipebst = skpipeline.Pipeline([('scaling', sc), ('feature_select', pca110), ('estimator', xgb_model)], memory=cachedir)

p_grid = [
    {
        'feature_select__n_components': [300,400,500],
        #'estimator__var_smoothing': [0.1,1,25,50]
        #'var_smoothing': [0.01,0.1,1,50]
        #'estimator__classifier': [nb],
        'estimator__n_estimators': [200],
        'estimator__min_samples_leaf': [5,50,100],
        #'estimator__min_samples_split': [20,50],
        #'estimator__max_depth': [20,40,60],
        #'n_estimators': [200,500],
        #'min_samples_leaf': [1000,2000,5000]
        #'estimator__hidden_layer_sizes': [(1000,)],
        #'estimator__alpha': [0.0001,1,100],
     
     # XGB Parameters
        #'estimator__learning_rate': [0.2],
        #'estimator__gamma': [0.05],
        #'estimator__max_depth': [50],
        #'estimator__min_child_weight': [5],
        #'estimator__scale_pos_weight': [14],
        #'estimator__subsample': [0.8],
        #'estimator__colsample_bytree': [0.6],
        #'estimator__colsample_bylevel': [0.9],
        #'estimator__colsample_bynode': [0.7],
    }
]

#roc_auc = metrics.make_scorer(metrics.roc_auc_score, average='weighted')
#roc_auc = metrics.make_scorer(metrics.roc_auc_score)
#precision = metrics.make_scorer(metrics.precision_score, average='weighted')
#accuracy = metrics.make_scorer(metrics.accuracy_score)
#recall = metrics.make_scorer(metrics.recall_score, average='weighted')
#fscore = metrics.make_scorer(metrics.f1_score)
accuracy=metrics.make_scorer(metrics.accuracy_score)
precision=metrics.make_scorer(metrics.precision_score, average='micro'
                              #average='weighted'
)
recall=metrics.make_scorer(metrics.recall_score, average='micro'
                              #average='weighted'
)
roc_auc=metrics.make_scorer(metrics.roc_auc_score, average='micro'
                              #average='weighted'
)
#scores = {'accuracy':accuracy, 'precision': precision, 'recall':recall, 'roc_auc':roc_auc}
scores = {'accuracy':accuracy, 'precision': precision, 'recall':recall}
#scores = {'precision':precision, 'recall':recall, 'roc_auc':roc_auc
#         }
scores = ['accuracy', 'precision', 'recall', 'roc_auc']
#scores = ['r2', 'neg_mean_squared_error', 'neg_mean_absolute_error', 'explained_variance']

"""
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=333, test_size = 0.75)
print('Gridsearching...')
clf = GridSearchCV(pipe3, param_grid=p_grid, cv=3, scoring=scores, refit=False, pre_dispatch=3)
#clf = GridSearchCV(pipe, param_grid=p_grid, cv=3, scoring='accuracy', refit=True, pre_dispatch=3)
clf.fit(X_train, y_train)
results = clf.cv_results_
#print(results)
#print(clf.grid_scores_)

#scores_df = pd.DataFrame(results).sort_values(by='rank_test_roc_auc')
scores_df = pd.DataFrame(results)
scores_df.to_csv('trees_search_reload.csv', sep=',', mode='w')
#files.download("scores_treepcanb.csv")
#print(scores_df)
from google.colab import files
files.download("trees_search_reload.csv")
print(scores_df)
"""

#sys.exit()

"""# Feature Importance

importance = clf.best_estimator_.named_steps['estimator'].feature_importances_
#importance = clf.named_steps['estimator'].feature_importances_
impdf = pd.DataFrame(importance, columns=['importance'])
impdf['features'] = features
impdf = impdf.sort_values(by='importance', ascending=False)

impdf.to_csv('treesansec_importance.csv', sep=',', mode='w')
files.download("treesansec_importance.csv")
#print(impdf)

# Dimension Reduction Comparison 

#import matplotlib.pyplot as plt
pca = pipe2.fit(X)
plt.plot(np.cumsum(pca.named_steps['estimator'].explained_variance_ratio_))
plt.xlabel('components')
plt.ylabel('cum variance')
print(np.cumsum(pca.named_steps['estimator'].explained_variance_ratio_))
sys.exit()
"""

##Production Model Parameters & Generate Prediction

# MODEL KEY PARAMETERS

pca_components=500
leaves=50
rstate=random.randint(1,1001)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rstate, test_size = 0.2)
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
npca = IncrementalPCA(n_components=pca_components)
X_train = npca.fit_transform(X_train)
model = ExtraTreesClassifier(oob_score=True, n_jobs=-1, random_state=rstate, bootstrap=True, class_weight='balanced', n_estimators=500, min_samples_leaf=leaves #10#, 
                             #min_samples_split=13
                            )
model.fit(X_train, y_train)

#"""
stock_datap=predict[['stock', 't']]
yp = predict['buy']
ypreturn = predict['nextd_return']

# Drop non-features for prediction data

#Xp = predict.drop(['buy_5d', 'buy_7d', 'buy_8d', 'buy_9d', 'buy_10d', 'buy_11d', #'buy_21d', #'buy_26d', 'buy_32d', 'buy_42d', 'buy', 'buy8up', 'buy11up', 'buy13up', 'buy16up', 'buy18up', 'buy21up', 'buy26up', 'buy32up', 
#                   'buy', 'stock', 't', 'buy_expand'#, 'buyall', 'buy8'
#                   ], axis=1)

#Xp = Xp.drop(['sell_5d', 'sell_7d', 'sell_8d', 'sell_9d', 'sell_10d', 'sell_11d'#, 'sell_21d'
#              ], axis=1)

Xp = predict.drop([#'buy_5d', 'buy_7d', 'buy_8d', 'buy_9d', 'buy_10d', 'buy_11d', #'buy_21d', 
              'buy', 'stock', 't', 'nextd_return'#, 'buyall', 'buy8'
              ], axis=1)

#for day in buy_windows:
#  Xp = Xp.drop(['{}{}{}'.format('buy_',day,'d'), '{}{}{}'.format('sell_',day,'d')], axis=1)

featuresp = Xp.columns.tolist()
Xp = sc.transform(Xp)
Xp = npca.transform(Xp)


dfpredict=stock_datap.loc[yp.index.values.tolist()]

dfpredict['y_proba']=model.predict_proba(Xp)[:,1]
dfpredict['y_proba-']=model.predict_proba(Xp)[:,0]
dfpredict['buy']=yp.loc[yp.index.values.tolist()]
dfpredict['return']=ypreturn.loc[yp.index.values.tolist()]

file_name="pca" + str(pca_components) + "reload"  + str(leaves) + "rfpred" + "_" + ending_date.strftime('%Y%m%d') + ".csv"
dfpredict=dfpredict.sort_values(by='y_proba', ascending=False)
dfpredict.to_csv(file_name, sep=',', mode='w')
from google.colab import files  
files.download(file_name)

print(model.classes_)
